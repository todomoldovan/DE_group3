{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd514d77-ed3c-425e-ac9a-99e83216df45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import unicodedata\n",
    "from pyspark import SparkContext,SparkConf,SQLContext\n",
    "from pyspark.sql import Row,SparkSession,HiveContext\n",
    "from pyspark.sql.functions import col,size,explode,split\n",
    "from pyspark.sql.types import StringType,IntegerType,ArrayType\n",
    "from pyspark.sql.functions import udf, array, length\n",
    "from bs4 import BeautifulSoup\n",
    "import mistune\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "spark = SparkSession.builder.appName(\"webis-tldr-17-corpus-normalization\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc)\n",
    "print(spark)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Clean the summarization corpus constructed from make_reddit\")\n",
    "parser.add_argument('--input_comments', type=str, help=\"HDFS path to Reddit comments\")\n",
    "parser.add_argument('--input_submissions', type=str, help=\"HDFS path to Reddit submissions\")\n",
    "parser.add_argument('--output_comments', type=str, help=\"HDFS path to save processed comments\")\n",
    "parser.add_argument('--output_submissions', type=str, help=\"HDFS path to save processed submissions\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_comments = str(args.input_comments)\n",
    "input_submissions = str(args.input_submissions)\n",
    "output_comments = str(args.output_comments)\n",
    "output_submissions = str(args.output_submissions)\n",
    "\n",
    "comments_df = spark.read.json(input_comments)\n",
    "submissions_df = spark.read.json(input_submissions)\n",
    "print(\"Initial number of comments: {}\".format(comments_df.count()))\n",
    "print(\"Initial number of submissions: {}\".format(submissions_df.count()))\n",
    "\n",
    "# To avoid recursion depth errors when using Mistune library for removing markdown\n",
    "sys.setrecursionlimit(300000)\n",
    "global markdownParser\n",
    "markdownParser = mistune.Markdown()\n",
    "global stop\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stop.update(['I', 'you', 'he', 'she', 'it', 'we', 'they', 'me','my' 'him', 'her', 'us', 'them'])\n",
    "\n",
    "def clean_text(input):\n",
    "    input = re.sub(r'http\\S+','',str(input))\n",
    "    input = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', input, flags=re.MULTILINE)\n",
    "    input = re.sub(r'&amp;', '', input)\n",
    "    input = re.sub(r'[_\"\\;%()|+&=*%:#$@\\[\\]/]', '', input)\n",
    "    input = re.sub('\\.\\.+', '.', input)\n",
    "    input = re.sub('\\!\\!+', '!', input)\n",
    "    input = re.sub('\\?\\?+', '?', input)\n",
    "    input = re.sub('\\-\\-+', '-', input)\n",
    "    parsed_text = ' '.join(BeautifulSoup(markdownParser(input),\"lxml\").findAll(text=True)).strip()\n",
    "    clean_text = unicodedata.normalize(\"NFKD\", parsed_text)\n",
    "    return clean_text\n",
    "\n",
    "def check_english(input):\n",
    "    words = input.lower().split()[0:10]\n",
    "    if stop.intersection(words):\n",
    "        return input\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "cleanText = udf(clean_text, StringType())\n",
    "checkEnglish = udf(check_english, StringType())\n",
    "removeSpecialCharsContent = udf(lambda input: re.sub(r\"\\W+$\", \"\", str(input)).strip(), StringType())\n",
    "removeSpecialCharsSummary = udf(lambda input: re.sub(r\"^\\W+\",\"\",str(input)).strip(), StringType())\n",
    "length_udf = udf(lambda input:len(input.strip().split()), IntegerType())\n",
    "\n",
    "\n",
    "comments_df = comments_df.withColumn('content',checkEnglish(comments_df.content))\n",
    "submissions_df = submissions_df.withColumn('content', checkEnglish(submissions_df.content))\n",
    "comments_df = comments_df.filter(comments_df.content.isNotNull())\n",
    "submissions_df = submissions_df.filter(submissions_df.content.isNotNull())\n",
    "print(\"After removing  non-english posts -> comments: {}\".format(comments_df.count()))\n",
    "print(\"After removing  non-english posts -> submissions : {}\".format(submissions_df.count()))\n",
    "\n",
    "comments_df = comments_df.withColumn('normalized_body',cleanText(comments_df.body))\n",
    "comments_df = comments_df.withColumn('normalized_content',cleanText(comments_df.content))\n",
    "comments_df = comments_df.withColumn('normalized_summary',cleanText(comments_df.summary))\n",
    "\n",
    "submissions_df = submissions_df.withColumn('normalized_selftext', cleanText(submissions_df.selftext))\n",
    "submissions_df = submissions_df.withColumn('normalized_content', cleanText(submissions_df.content))\n",
    "submissions_df = submissions_df.withColumn('normalized_summary', cleanText(submissions_df.summary))\n",
    "\n",
    "comments_df = comments_df.withColumn('normalized_content', removeSpecialCharsContent(comments_df.normalized_content))\n",
    "submissions_df = submissions_df.withColumn('normalized_content', removeSpecialCharsContent(submissions_df.normalized_content))\n",
    "comments_df = comments_df.withColumn('normalized_summary', removeSpecialCharsSummary(comments_df.normalized_summary))\n",
    "submissions_df = submissions_df.withColumn('normalized_summary', removeSpecialCharsSummary(submissions_df.normalized_summary))\n",
    "\n",
    "comments_df = comments_df.withColumn('content_len',length_udf(comments_df.normalized_content))\n",
    "comments_df = comments_df.withColumn('summary_len',length_udf(comments_df.normalized_summary))\n",
    "comments_df = comments_df.withColumn('body_len',length_udf(comments_df.normalized_body))\n",
    "\n",
    "submissions_df = submissions_df.withColumn('content_len', length_udf(submissions_df.normalized_content))\n",
    "submissions_df = submissions_df.withColumn('summary_len', length_udf(submissions_df.normalized_summary))\n",
    "submissions_df = submissions_df.withColumn('selftext_len', length_udf(submissions_df.normalized_selftext))\n",
    "\n",
    "comments_df = comments_df.where(comments_df['content_len']>comments_df['summary_len'])\n",
    "submissions_df = submissions_df.where(submissions_df['content_len']>submissions_df['summary_len'])\n",
    "comments_df = comments_df.where(comments_df['summary_len']>0)\n",
    "submissions_df = submissions_df.where(submissions_df['summary_len']>0)\n",
    "\n",
    "\n",
    "print(\"Saving cleaned comments to {}\".format(output_comments))\n",
    "comments_df.write.json(output_comments)\n",
    "print(\"Done\")\n",
    "print(\"Saving cleaned submissions to {}\".format(output_submissions))\n",
    "submissions_df.write.json(output_submissions)\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e603a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import argparse\n",
    "import unicodedata\n",
    "from pyspark import SparkContext,SparkConf,SQLContext\n",
    "from pyspark.sql import Row,SparkSession,HiveContext\n",
    "from pyspark.sql.functions import col,size,explode,split\n",
    "from pyspark.sql.types import StringType,IntegerType,ArrayType\n",
    "from pyspark.sql.functions import udf, array, length\n",
    "from bs4 import BeautifulSoup\n",
    "import mistune\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "spark = SparkSession.builder.appName(\"webis-tldr-17-corpus-normalization\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "print(sc)\n",
    "print(spark)\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Clean the summarization corpus constructed from make_reddit\")\n",
    "parser.add_argument('--input_comments', type=str, help=\"HDFS path to Reddit comments\")\n",
    "parser.add_argument('--input_submissions', type=str, help=\"HDFS path to Reddit submissions\")\n",
    "parser.add_argument('--output_comments', type=str, help=\"HDFS path to save processed comments\")\n",
    "parser.add_argument('--output_submissions', type=str, help=\"HDFS path to save processed submissions\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "input_comments = str(args.input_comments)\n",
    "input_submissions = str(args.input_submissions)\n",
    "output_comments = str(args.output_comments)\n",
    "output_submissions = str(args.output_submissions)\n",
    "\n",
    "comments_df = spark.read.json(input_comments)\n",
    "submissions_df = spark.read.json(input_submissions)\n",
    "print(\"Initial number of comments: {}\".format(comments_df.count()))\n",
    "print(\"Initial number of submissions: {}\".format(submissions_df.count()))\n",
    "\n",
    "# To avoid recursion depth errors when using Mistune library for removing markdown\n",
    "sys.setrecursionlimit(300000)\n",
    "global markdownParser\n",
    "markdownParser = mistune.Markdown()\n",
    "global stop\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stop.update(['I', 'you', 'he', 'she', 'it', 'we', 'they', 'me','my' 'him', 'her', 'us', 'them'])\n",
    "\n",
    "def clean_text(input):\n",
    "    input = re.sub(r'http\\S+','',str(input))\n",
    "    input = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', input, flags=re.MULTILINE)\n",
    "    input = re.sub(r'&amp;', '', input)\n",
    "    input = re.sub(r'[_\"\\;%()|+&=*%:#$@\\[\\]/]', '', input)\n",
    "    input = re.sub('\\.\\.+', '.', input)\n",
    "    input = re.sub('\\!\\!+', '!', input)\n",
    "    input = re.sub('\\?\\?+', '?', input)\n",
    "    input = re.sub('\\-\\-+', '-', input)\n",
    "    parsed_text = ' '.join(BeautifulSoup(markdownParser(input),\"lxml\").findAll(text=True)).strip()\n",
    "    clean_text = unicodedata.normalize(\"NFKD\", parsed_text)\n",
    "    return clean_text\n",
    "\n",
    "def check_english(input):\n",
    "    words = input.lower().split()[0:10]\n",
    "    if stop.intersection(words):\n",
    "        return input\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "cleanText = udf(clean_text, StringType())\n",
    "checkEnglish = udf(check_english, StringType())\n",
    "removeSpecialCharsContent = udf(lambda input: re.sub(r\"\\W+$\", \"\", str(input)).strip(), StringType())\n",
    "removeSpecialCharsSummary = udf(lambda input: re.sub(r\"^\\W+\",\"\",str(input)).strip(), StringType())\n",
    "length_udf = udf(lambda input:len(input.strip().split()), IntegerType())\n",
    "\n",
    "\n",
    "comments_df = comments_df.withColumn('content',checkEnglish(comments_df.content))\n",
    "submissions_df = submissions_df.withColumn('content', checkEnglish(submissions_df.content))\n",
    "comments_df = comments_df.filter(comments_df.content.isNotNull())\n",
    "submissions_df = submissions_df.filter(submissions_df.content.isNotNull())\n",
    "print(\"After removing  non-english posts -> comments: {}\".format(comments_df.count()))\n",
    "print(\"After removing  non-english posts -> submissions : {}\".format(submissions_df.count()))\n",
    "\n",
    "comments_df = comments_df.withColumn('normalized_body',cleanText(comments_df.body))\n",
    "comments_df = comments_df.withColumn('normalized_content',cleanText(comments_df.content))\n",
    "comments_df = comments_df.withColumn('normalized_summary',cleanText(comments_df.summary))\n",
    "\n",
    "submissions_df = submissions_df.withColumn('normalized_selftext', cleanText(submissions_df.selftext))\n",
    "submissions_df = submissions_df.withColumn('normalized_content', cleanText(submissions_df.content))\n",
    "submissions_df = submissions_df.withColumn('normalized_summary', cleanText(submissions_df.summary))\n",
    "\n",
    "comments_df = comments_df.withColumn('normalized_content', removeSpecialCharsContent(comments_df.normalized_content))\n",
    "submissions_df = submissions_df.withColumn('normalized_content', removeSpecialCharsContent(submissions_df.normalized_content))\n",
    "comments_df = comments_df.withColumn('normalized_summary', removeSpecialCharsSummary(comments_df.normalized_summary))\n",
    "submissions_df = submissions_df.withColumn('normalized_summary', removeSpecialCharsSummary(submissions_df.normalized_summary))\n",
    "\n",
    "comments_df = comments_df.withColumn('content_len',length_udf(comments_df.normalized_content))\n",
    "comments_df = comments_df.withColumn('summary_len',length_udf(comments_df.normalized_summary))\n",
    "comments_df = comments_df.withColumn('body_len',length_udf(comments_df.normalized_body))\n",
    "\n",
    "submissions_df = submissions_df.withColumn('content_len', length_udf(submissions_df.normalized_content))\n",
    "submissions_df = submissions_df.withColumn('summary_len', length_udf(submissions_df.normalized_summary))\n",
    "submissions_df = submissions_df.withColumn('selftext_len', length_udf(submissions_df.normalized_selftext))\n",
    "\n",
    "comments_df = comments_df.where(comments_df['content_len']>comments_df['summary_len'])\n",
    "submissions_df = submissions_df.where(submissions_df['content_len']>submissions_df['summary_len'])\n",
    "comments_df = comments_df.where(comments_df['summary_len']>0)\n",
    "submissions_df = submissions_df.where(submissions_df['summary_len']>0)\n",
    "\n",
    "\n",
    "print(\"Saving cleaned comments to {}\".format(output_comments))\n",
    "comments_df.write.json(output_comments)\n",
    "print(\"Done\")\n",
    "print(\"Saving cleaned submissions to {}\".format(output_submissions))\n",
    "submissions_df.write.json(output_submissions)\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
