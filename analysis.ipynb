{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1668aa-8e5e-42db-9f1a-cc05f1794588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c74c189-f4ca-4413-a439-7c83259b773d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/08 12:09:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg, length\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.219:7077\") \\\n",
    "    .appName(\"YourAppName\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5019acf2-53d0-4eba-8415-07432c976af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_path = \"hdfs://192.168.2.219:9001/user/root/reddit_data/corpus-webis-tldr-17.json\"\n",
    "df_old = spark.read.json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8984796-ea47-44c8-8d69-f24b871ced15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_old.limit(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fec3fc2-593a-4a43-8736-0b43117acdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.cache()\n",
    "nrows = df.count() # force caching\n",
    "    \n",
    "# need to access hidden parameters from the `SparkSession` and `DataFrame`\n",
    "size_bytes = sc._jvm.org.apache.spark.util.SizeEstimator.estimate(df._jdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63864ae5-86c0-4481-9ebb-cf9d2b9b1cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+------------------+\n",
      "|          subreddit|avg_content_length|avg_summary_length|\n",
      "+-------------------+------------------+------------------+\n",
      "|              0x10c|1034.4545454545455| 243.1818181818182|\n",
      "|         0x10cships|             869.0|              44.0|\n",
      "|1000thworldproblems|              14.0|               6.0|\n",
      "|         100pushups|             578.0|              67.5|\n",
      "|            100sets|            2692.0|              44.0|\n",
      "|       1200isplenty|1109.8333333333333|238.16666666666666|\n",
      "|            13thage|            1622.0|             153.0|\n",
      "|     14TeamStandard|             475.0|              27.0|\n",
      "|               1911|             868.4|             121.4|\n",
      "|          2007scape| 704.9596412556053| 93.37219730941705|\n",
      "|      2012Elections|             970.2|              52.0|\n",
      "|      2020elections|            3009.0|             448.0|\n",
      "|               2048|            2276.0|              58.5|\n",
      "|           20xxgame|             434.0|              57.0|\n",
      "|              240sx| 665.6363636363636| 92.27272727272727|\n",
      "|               24OP|            1791.0|              82.0|\n",
      "|      24hoursupport|         1359.8125|          118.3125|\n",
      "|               250r| 982.3333333333334|             214.0|\n",
      "|               29er|             758.5|              81.0|\n",
      "|  2HourTrackSundays|            1708.0|              89.0|\n",
      "+-------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "CPU times: user 255 ms, sys: 21.7 ms, total: 276 ms\n",
      "Wall time: 3min 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"content_length\", length(\"content\"))\n",
    "df = df.withColumn(\"summary_length\", length(\"summary\"))\n",
    "# Group by subreddit and calculate average lengths\n",
    "average_lengths = df.groupBy(\"subreddit\").agg(\n",
    "    avg(\"content_length\").alias(\"avg_content_length\"),\n",
    "    avg(\"summary_length\").alias(\"avg_summary_length\")\n",
    ")\n",
    "# Order by subreddit for readability (optional, can be removed for large datasets)\n",
    "average_lengths = average_lengths.orderBy(\"subreddit\")\n",
    "# Show the results\n",
    "average_lengths.show()\n",
    "# Optionally, save the results to a file\n",
    "output_path = \"/home/ubuntu/Drive/reddit_data/output/average_lengths.csv\"\n",
    "average_lengths.write.csv(output_path, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322876b5-6e8e-4d82-86cd-75dc76c348f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c12dc4a-e0b9-4c09-8f56-36ab333eec60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/08 12:06:22 WARN TransportChannelHandler: Exception in connection from /192.168.2.219:55494\n",
      "java.io.IOException: Connection reset by peer\n",
      "\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n",
      "\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n",
      "\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n",
      "\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n",
      "\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n",
      "\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:254)\n",
      "\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1132)\n",
      "\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:357)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:151)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "24/03/08 12:06:22 WARN TaskSetManager: Lost task 62.0 in stage 3.3 (TID 648) (192.168.2.219 executor 4): TaskKilled (Stage cancelled: Job aborted due to stage failure: Master removed our application: KILLED)\n",
      "24/03/08 12:06:22 ERROR Utils: Uncaught exception in thread stop-spark-context0]\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient.stop(StandaloneAppClient.scala:288)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:275)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:142)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.SchedulerBackend.stop$(SchedulerBackend.scala:33)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:54)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:992)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2976)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1375)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2216)\n",
      "\tat org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:2203)\n",
      "Caused by: org.apache.spark.SparkException: Could not find AppClient.\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:178)\n",
      "\tat org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:554)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:558)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:72)\n",
      "\t... 17 more\n"
     ]
    }
   ],
   "source": [
    "average_lenghts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
